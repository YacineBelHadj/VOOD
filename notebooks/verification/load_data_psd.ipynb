{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from config import settings , parse_datetime_strings\n",
    "from pathlib import Path\n",
    "psd_nperseg = 8192\n",
    "path_psd = Path(settings.default.path['processed_data']) / f'PSD_{psd_nperseg}.parquet'\n",
    "table = pq.read_table(path_psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load only the time column\n",
    "time = table.column('time').to_pandas()\n",
    "datetime_range = parse_datetime_strings(settings.split['train'])\n",
    "start, end = datetime_range['start'].timestamp(), datetime_range['end'].timestamp()\n",
    "\n",
    "time = time.index[(time >= start) & (time <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACCX'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sensor_axis('ACCX_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import cached_property\n",
    "\n",
    "def get_sensor_axis(sensor_name: str):\n",
    "    sensor_axis = sensor_name.split('_')[0][-1]\n",
    "    return sensor_axis\n",
    "def get_sensor_position(sensor_name: str):\n",
    "    sensor_position = sensor_name.split('_')[1]\n",
    "    return sensor_position\n",
    "\n",
    "class PSDDataset(Dataset):\n",
    "    def __init__(self, filename: Union[Path, str], datetime_range: Tuple[datetime, datetime],\n",
    "                 drop: List[str] = None, transform=None, label_transform=None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        self.drop = set(drop) if drop else set()\n",
    "\n",
    "        # Open the parquet file\n",
    "        self.parquet_file = pq.ParquetFile(filename)\n",
    "        \n",
    "        # Get keys within the datetime range\n",
    "        time = pq.read_table(filename).column('time').to_pandas()\n",
    "        self.keys = time.index[(time >= datetime_range[0].timestamp()) & (time <= datetime_range[1].timestamp())]\n",
    "\n",
    "        self.initialize_dicts()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def initialize_dicts(self):\n",
    "        psd_group = self.parquet_file.read_row_group(self.keys[0])\n",
    "        sensor_names = set(np.unique(psd_group['sensor_name'].to_pandas().values[0]))\n",
    "        \n",
    "        # Filter out the sensors not in drop\n",
    "        sensor_names = {sensor for sensor in sensor_names if sensor not in self.drop}\n",
    "\n",
    "        axis_names = set(map(get_sensor_axis, sensor_names))\n",
    "        position_names = set(map(get_sensor_position, sensor_names))\n",
    "\n",
    "        # Store them as attributes\n",
    "        self._sensor_names = list(sensor_names)\n",
    "        self._axis_names = list(axis_names)\n",
    "        self._position_names = list(position_names)\n",
    "        \n",
    "        self._mapping_dict = {sensor: i for i, sensor in enumerate(self._sensor_names)}\n",
    "        self._mapping_position_dict = {position: i for i, position in enumerate(self._position_names)}\n",
    "        self._mapping_axis_dict = {axis: i for i, axis in enumerate(self._axis_names)}\n",
    "    \n",
    "    @cached_property\n",
    "    def sensor_names(self) -> List[str]:\n",
    "        return self._sensor_names\n",
    "\n",
    "    @cached_property\n",
    "    def axis_names(self) -> List[str]:\n",
    "        return self._axis_names\n",
    "\n",
    "    @cached_property\n",
    "    def position_names(self) -> List[str]:\n",
    "        return self._position_names\n",
    "\n",
    "    @cached_property\n",
    "    def mapping_position_dict(self) -> dict:\n",
    "        return self._mapping_position_dict\n",
    "\n",
    "    @cached_property\n",
    "    def mapping_axis_dict(self) -> dict:\n",
    "        return self._mapping_axis_dict\n",
    "\n",
    "    @cached_property\n",
    "    def mapping_dict(self) -> dict:\n",
    "        return self._mapping_dict\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        key = self.keys[index]\n",
    "        \n",
    "        # Read the required row group\n",
    "        sample = self.parquet_file.read_row_group(key).to_pandas()\n",
    "        \n",
    "        psd = sample['ACC_psd'].values[0]\n",
    "        sensor_name = sample['sensor_name'].values[0]\n",
    "        psd = psd.reshape(len(sensor_name), -1)\n",
    "\n",
    "        # Filter out the rows based on the `drop` attribute\n",
    "        keep_rows = ~np.isin(sensor_name, list(self.drop))\n",
    "        psd = psd[keep_rows]\n",
    "        sensor_name = sensor_name[keep_rows]\n",
    "        sensor_name = np.array([self._mapping_dict[sensor] for sensor in sensor_name])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        psd = torch.from_numpy(psd)\n",
    "        sensor_name = torch.from_numpy(sensor_name)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform is not None:\n",
    "            psd = self.transform(psd)\n",
    "        if self.label_transform is not None:\n",
    "            sensor_name = self.label_transform(sensor_name)\n",
    "        \n",
    "        return psd, sensor_name\n",
    "\n",
    "# Example usage\n",
    "from config import settings, parse_datetime_strings\n",
    "\n",
    "range_dt = parse_datetime_strings(settings.split['train'])\n",
    "training_range = (range_dt['start'], range_dt['end'])\n",
    "dataset = PSDDataset(path_psd, training_range)\n",
    "dataloader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "s = start \n",
    "for i in dataloader:\n",
    "    print('time',time()-start)\n",
    "    start = time()\n",
    "print('total time',time()-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OOD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
